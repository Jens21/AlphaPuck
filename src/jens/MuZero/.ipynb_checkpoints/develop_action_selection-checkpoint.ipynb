{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27a1ce76-4e2d-4705-8f13-e985eaa222b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import laserhockey.hockey_env as h_env\n",
    "\n",
    "from own_env import OwnEnv\n",
    "from action_selection import ActionSelection\n",
    "from utils import ACTIONS_T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e21a22a-d8f7-4953-91ea-cf7c3e504a0a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ResidualBlock(th.nn.Module):\n",
    "    def __init__(self, inp_out_dim, hidden_dim=128):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.inp_out_dim = inp_out_dim\n",
    "\n",
    "        self.model = th.nn.Sequential(\n",
    "            th.nn.Linear(inp_out_dim, hidden_dim),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Linear(hidden_dim, inp_out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.model(x)\n",
    "\n",
    "        return out + x\n",
    "\n",
    "\n",
    "class Network(th.nn.Module):\n",
    "    def __init__(self, obs_dim=18, action_dim=4):\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.obs_dim = obs_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # Dynamic network for predicting next observation and reward\n",
    "        self.model_repres = th.nn.Sequential(\n",
    "            th.nn.Linear(obs_dim, 128),\n",
    "            # th.nn.BatchNorm1d(128),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Linear(128, 64),\n",
    "            th.nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.model_dynamic_linear = th.nn.Sequential(\n",
    "            th.nn.Linear(64 + 2 * action_dim, 64)\n",
    "        )\n",
    "        self.model_dynamic_res_block = ResidualBlock(inp_out_dim=64, hidden_dim=128)\n",
    "\n",
    "        self.model_reward = th.nn.Sequential(\n",
    "            th.nn.Linear(64 + 2 * action_dim, 128),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Linear(128, 3)\n",
    "        )\n",
    "\n",
    "        self.model_state_value = th.nn.Sequential(\n",
    "            th.nn.Linear(64, 128),\n",
    "            th.nn.ReLU(),\n",
    "            th.nn.Linear(128, 1+ACTIONS_T.shape[0]),\n",
    "            th.nn.Tanh()\n",
    "        )\n",
    "\n",
    "        self.observation_mean = th.FloatTensor([[-2.07, 0, 0, 0, 0, 0, 2.07, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "        self.observation_std = th.FloatTensor([[1.57, 2.91, 1.04, 4, 4, 6, 1.57, 2.91, 1.04, 4, 4, 6, 3.7, 3, 12, 12, 15, 15]])\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError('This function should not be used!')\n",
    "\n",
    "    def initial_inference(self, obs):\n",
    "        obs_in = (obs - self.observation_mean) / self.observation_std\n",
    "\n",
    "        latent_state = self.model_repres(obs_in)\n",
    "        out = self.model_state_value(latent_state)\n",
    "        state_values, policy_logits = out[..., 0], out[..., 1:]\n",
    "\n",
    "        return latent_state, state_values, policy_logits\n",
    "\n",
    "    def recurrent_inference(self, latent_state, action_1, action_2):\n",
    "        next_latent_state = self.forward_dynamic(latent_state, action_1, action_2)\n",
    "\n",
    "        out = self.model_state_value(next_latent_state)\n",
    "        next_state_values, policy_logits = out[..., 0], out[..., 1:]\n",
    "\n",
    "        net_inp = th.concat([latent_state, action_1, action_2], dim=1)\n",
    "        rewards_logits = self.model_reward(net_inp)\n",
    "        rewards = th.FloatTensor([-1, 0, 1])[rewards_logits.argmax(dim=-1)]\n",
    "\n",
    "        return next_latent_state, rewards, rewards_logits, next_state_values, policy_logits\n",
    "\n",
    "    def forward_dynamic(self, latent_state, action_1, action_2):\n",
    "        net_inp = th.concat([latent_state, action_1, action_2], dim=1)\n",
    "        state = self.model_dynamic_linear(net_inp)\n",
    "        state = F.relu(state + latent_state)\n",
    "        state = self.model_dynamic_res_block(state)\n",
    "\n",
    "        return F.tanh(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "6a70a001-af5b-4acd-9962-874b7eccd8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OwnEnv()\n",
    "opponent = h_env.BasicOpponent(weak=True)\n",
    "net = Network().eval()\n",
    "net.load_state_dict(th.load('checkpoints/agent_1250000.pth'))\n",
    "action_selection = ActionSelection(0.95, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "id": "c17b463d-e6b0-4247-bf48-e66854036979",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_BEST = 10\n",
    "N_TIMES = 2\n",
    "gamma = 0.95\n",
    "\n",
    "n_actions = ACTIONS_T.shape[0]\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, latent_state, state_value, policy):\n",
    "        self.state_value = state_value\n",
    "        self.latent_state = latent_state\n",
    "        self.R = th.zeros(n_actions, dtype=th.float)\n",
    "        self.S = n_actions * [None]\n",
    "        self.N = th.zeros(n_actions, dtype=th.float)\n",
    "        self.Q = th.zeros(n_actions, dtype=th.float)\n",
    "        self.P = policy        \n",
    "\n",
    "class TreeBuilder():\n",
    "    def __init__(self, n_simulations):\n",
    "        self.n_simulations = n_simulations\n",
    "\n",
    "    def build_tree(self, obs, net):\n",
    "        obs_in = th.from_numpy(obs)[None].float()\n",
    "        latent_state, state_value, policy_logits = net.initial_inference(obs_in)\n",
    "        policy = th.softmax(policy_logits[0], dim=0)\n",
    "        \n",
    "        root_node = Node(latent_state, state_value[0], policy)\n",
    "\n",
    "        for _ in range(self.n_simulations):\n",
    "            l_nodes, l_action_indices = self.selection(root_node)\n",
    "            last_node, last_action_idx = l_nodes[-1], l_action_indices[-1]\n",
    "            self.expansion(net, last_node, last_action_idx)\n",
    "            self.backup()\n",
    "\n",
    "        raise NotImplementedError()\n",
    "        return root_node\n",
    "\n",
    "    def selection(self, root_node):\n",
    "        l_nodes = [root_node]\n",
    "        l_action_indices = []\n",
    "        \n",
    "        while True:\n",
    "            node = l_nodes[-1]\n",
    "            n_sum = th.maximum(node.N.sum(), th.FloatTensor([1]))\n",
    "            \n",
    "            ucb = node.Q + 1.25 * node.P * th.sqrt(n_sum) / (1 + node.N)\n",
    "            action_idx = ucb.argmax()\n",
    "            l_action_indices.append(action_idx)\n",
    "\n",
    "            if node.S[action_idx] is None:\n",
    "                break\n",
    "            else:\n",
    "                l_nodes.append(node.S[action_idx])\n",
    "        \n",
    "        return l_nodes, l_action_indices\n",
    "\n",
    "    def expansion(self, net, node, action_idx):\n",
    "        action_1 = ACTIONS_T[action_idx][None].tile(n_actions, 1)\n",
    "        action_2 = ACTIONS_T\n",
    "        latent_inp = node.latent_state.tile(n_actions, 1)\n",
    "        \n",
    "        next_latent_state, rewards, _, next_state_values, policy_logits = net.recurrent_inference(latent_inp, action_1, action_2)\n",
    "        \n",
    "        min_idx = next_state_values.argmin()\n",
    "        latent_state = next_latent_state[min_idx][None]\n",
    "        value = next_state_values[min_idx]\n",
    "        policy = th.softmax(policy_logits[min_idx], dim=0)\n",
    "\n",
    "        child_node = Node(latent_state, value, policy)\n",
    "        \n",
    "        node.R[action_idx] = rewards[min_idx]\n",
    "        node.S[action_idx] = child_node\n",
    "\n",
    "        return value\n",
    "\n",
    "        \n",
    "    def backup(self):\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "\n",
    "tree_builder = TreeBuilder(50)\n",
    "def exploitation_v4(net, obs):\n",
    "    net.eval()\n",
    "\n",
    "    with th.no_grad():\n",
    "        root_node = tree_builder.build_tree(obs, net)\n",
    "        \n",
    "        return max_action_idx, policy_distr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "id": "d2e7da53-aee1-436f-a822-6f9c50ed5371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([25])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:10\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "Input \u001b[0;32mIn [490]\u001b[0m, in \u001b[0;36mexploitation_v4\u001b[0;34m(net, obs)\u001b[0m\n\u001b[1;32m     79\u001b[0m net\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 82\u001b[0m     root_node \u001b[38;5;241m=\u001b[39m \u001b[43mtree_builder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m max_action_idx, policy_distr\n",
      "Input \u001b[0;32mIn [490]\u001b[0m, in \u001b[0;36mTreeBuilder.build_tree\u001b[0;34m(self, obs, net)\u001b[0m\n\u001b[1;32m     29\u001b[0m     l_nodes, l_action_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselection(root_node)\n\u001b[1;32m     30\u001b[0m     last_node, last_action_idx \u001b[38;5;241m=\u001b[39m l_nodes[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], l_action_indices[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpansion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlast_action_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackup()\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "Input \u001b[0;32mIn [490]\u001b[0m, in \u001b[0;36mTreeBuilder.expansion\u001b[0;34m(self, net, node, action_idx)\u001b[0m\n\u001b[1;32m     67\u001b[0m policy \u001b[38;5;241m=\u001b[39m th\u001b[38;5;241m.\u001b[39msoftmax(policy_logits[min_idx], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(policy\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m()\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "obs1, _ = env.reset()\n",
    "obs2 = env.obs_agent_two()\n",
    "l_obs = [obs1]\n",
    "\n",
    "im = 0\n",
    "while True:\n",
    "    # env.render()\n",
    "    # max_action_idx, policy_distr = action_selection.exploitation_v2(net, obs1)\n",
    "    # max_action_idx, policy_distr = exploitation_v3(net, obs1)\n",
    "    max_action_idx, policy_distr = exploitation_v4(net, obs1)\n",
    "    action1 = ACTIONS_T[max_action_idx]\n",
    "    action2 = opponent.act(obs2)\n",
    "\n",
    "    obs1, rew, done, trunc, info = env.step(np.hstack([action1, action2]))\n",
    "    obs2 = env.obs_agent_two()\n",
    "    l_obs.append(obs1)\n",
    "\n",
    "    im += 1\n",
    "\n",
    "    if trunc:\n",
    "        break\n",
    "\n",
    "print(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "f52381d0-ac2c-47e7-b65f-fa00c94befc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.4211, 10.5620, 10.9890, 11.1034, 10.6201, 10.0202, 10.1682, 10.2735,\n",
      "        10.3625, 10.5093, 11.1403, 10.9367, 10.9626, 10.4226,  9.9553, 10.8211,\n",
      "        11.1367, 10.4718, 10.7605, 10.9001, 10.9816, 10.4303,  9.9579, 10.1462,\n",
      "        10.4361], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "obs = l_obs[0]\n",
    "\n",
    "max_action_idx, policy_distr, latent_states, search_depths, value_prefixes, action_indices, state_values = exploitation_v2(net, obs)\n",
    "\n",
    "print(latent_states.sum(dim=1))\n",
    "# print(state_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "76fdcb3a-f3c5-47fe-95ea-7d6ab1570864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2055, grad_fn=<AsStridedBackward0>) 16 tensor(10.4211, grad_fn=<SumBackward0>)\n",
      "tensor(0.2244, grad_fn=<AsStridedBackward0>) 16 tensor(10.5620, grad_fn=<SumBackward0>)\n",
      "tensor(0.2241, grad_fn=<AsStridedBackward0>) 16 tensor(10.9890, grad_fn=<SumBackward0>)\n",
      "tensor(0.2063, grad_fn=<AsStridedBackward0>) 16 tensor(11.1034, grad_fn=<SumBackward0>)\n",
      "tensor(0.2018, grad_fn=<AsStridedBackward0>) 16 tensor(10.6201, grad_fn=<SumBackward0>)\n",
      "tensor(0.2015, grad_fn=<AsStridedBackward0>) 16 tensor(10.0202, grad_fn=<SumBackward0>)\n",
      "tensor(0.1716, grad_fn=<AsStridedBackward0>) 2 tensor(10.1682, grad_fn=<SumBackward0>)\n",
      "tensor(0.1989, grad_fn=<AsStridedBackward0>) 16 tensor(10.2735, grad_fn=<SumBackward0>)\n",
      "tensor(0.2047, grad_fn=<AsStridedBackward0>) 8 tensor(10.3625, grad_fn=<SumBackward0>)\n",
      "tensor(0.1875, grad_fn=<AsStridedBackward0>) 16 tensor(10.5093, grad_fn=<SumBackward0>)\n",
      "tensor(0.1850, grad_fn=<AsStridedBackward0>) 9 tensor(11.1403, grad_fn=<SumBackward0>)\n",
      "tensor(0.1850, grad_fn=<AsStridedBackward0>) 16 tensor(10.9367, grad_fn=<SumBackward0>)\n",
      "tensor(0.1740, grad_fn=<AsStridedBackward0>) 10 tensor(10.9626, grad_fn=<SumBackward0>)\n",
      "tensor(0.1679, grad_fn=<AsStridedBackward0>) 10 tensor(10.4226, grad_fn=<SumBackward0>)\n",
      "tensor(0.1518, grad_fn=<AsStridedBackward0>) 10 tensor(9.9553, grad_fn=<SumBackward0>)\n",
      "tensor(0.1459, grad_fn=<AsStridedBackward0>) 10 tensor(10.8211, grad_fn=<SumBackward0>)\n",
      "tensor(0.1734, grad_fn=<AsStridedBackward0>) 3 tensor(11.1367, grad_fn=<SumBackward0>)\n",
      "tensor(0.2137, grad_fn=<AsStridedBackward0>) 8 tensor(10.4718, grad_fn=<SumBackward0>)\n",
      "tensor(0.2220, grad_fn=<AsStridedBackward0>) 16 tensor(10.7605, grad_fn=<SumBackward0>)\n",
      "tensor(0.2163, grad_fn=<AsStridedBackward0>) 16 tensor(10.9001, grad_fn=<SumBackward0>)\n",
      "tensor(0.1991, grad_fn=<AsStridedBackward0>) 10 tensor(10.9816, grad_fn=<SumBackward0>)\n",
      "tensor(0.1790, grad_fn=<AsStridedBackward0>) 10 tensor(10.4303, grad_fn=<SumBackward0>)\n",
      "tensor(0.1545, grad_fn=<AsStridedBackward0>) 2 tensor(9.9579, grad_fn=<SumBackward0>)\n",
      "tensor(0.1850, grad_fn=<AsStridedBackward0>) 8 tensor(10.1462, grad_fn=<SumBackward0>)\n",
      "tensor(0.1855, grad_fn=<AsStridedBackward0>) 1 tensor(10.4361, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs_in = th.from_numpy(obs)[None].float()\n",
    "latent_states, _, _ = net.initial_inference(obs_in)\n",
    "# for j in range(25):\n",
    "#     latent_out, rewards, _, next_state_values, _ = net.recurrent_inference(latent_states, ACTIONS_T[1][None], ACTIONS_T[16][None])\n",
    "\n",
    "\n",
    "#     print(latent_out.sum()==)\n",
    "t = th.empty((25, 25))\n",
    "for i in range(n_actions):\n",
    "    min_value = th.FloatTensor([1e9])\n",
    "    min_idx = -1\n",
    "    min_latent_state = 0\n",
    "    for j in range(n_actions):\n",
    "        latent_out, rewards, _, next_state_values, _ = net.recurrent_inference(latent_states, ACTIONS_T[i][None], ACTIONS_T[j][None])\n",
    "\n",
    "        t[i,j] = next_state_values*gamma\n",
    "        if t[i,j]<min_value:\n",
    "            min_value=t[i,j]\n",
    "            min_idx = j\n",
    "            min_latent_state = latent_out\n",
    "    print(min_value, min_idx, min_latent_state.sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "b1500c1f-31dc-44fa-89e4-eff4eb04959d",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
